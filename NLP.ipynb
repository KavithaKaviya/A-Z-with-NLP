{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEGA BLOG POST - Natural Language Processing\n",
    "\n",
    "This notebook endeavors a long-term effort of mine, to be the ultimate guide to standard Web Mining and NLP problems, for reference for developers.\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "Who am I? \n",
    "\n",
    "I'm a Former Intern at Cisco and Deloitte. \n",
    "\n",
    "I'm a published algorithm blogger on Topcoder, and I run my own personal blog on Algorithms too. (So I'm no stranger to technical writing ðŸ˜‰)  \n",
    "\n",
    "I frequently compete in programming contests. I'm a computer science undergrad who loves programming.\n",
    "\n",
    "You can find more of me here -\n",
    "* My Linkedin - https://www.linkedin.com/in/adityaramesh1998\n",
    "* My Portfolio Website and Contact - https://www.rameshaditya.com/\n",
    "* My GitHub Profile - https://www.github.com/RameshAditya/\n",
    "\n",
    "**This notebook will also be uploaded on my GitHub Profile, so you might want to hit \"star\" and \"watch\" to stay tuned for updates on GitHub. ** ðŸ™‚\n",
    "\n",
    "Do follow me on GitHub and connect with me on LinkedIn to follow me for more of my work! ðŸ™‚\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "You could support me in making more work like this by buying me coffee. \n",
    "\n",
    "<a href=\"https://www.buymeacoffee.com/adityaramesh\" target=\"_blank\"><img src=\"https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png\" alt=\"Buy Me A Coffee\" style=\"height: auto !important;width: auto !important;\" ></a>\n",
    "\n",
    "\n",
    "(*No seriously, I'd like some <strike>coffee</strike> support.* ðŸ˜›)\n",
    "\n",
    "\n",
    "Current contents include (in suggested order of reading) -\n",
    "* [Web Scraping - Extracting Text From Target Website](#Web-Scraping---Extracting-Text-From-Target-Website)\n",
    "\n",
    "\n",
    "* [Tokenizing](#Tokenizing)\n",
    "\n",
    "\n",
    "* [Stopword Removal](#Stopword-Removal)\n",
    "\n",
    "\n",
    "* [Stemming Text](#Word-Stemming)\n",
    "\n",
    "\n",
    "* [Document Representation](#Document-Representation)\n",
    "    - [Boolean Matrix](#Boolean-Matrix)\n",
    "    - [Term Frequency Matrix (Bag Of Words Model)](#Term-Frequency-Matrix-or-Bag-Of-Words-Model)\n",
    "    - [Complete Representation](#Complete-Representation)\n",
    "    \n",
    "\n",
    "* [IDF and TF-IDF](#IDF-and-TF-IDF)\n",
    "\n",
    "\n",
    "* [Similarity](#Similarity)\n",
    "    - [Euclidean Similarity](#Euclidean-Similarity)\n",
    "    - [Cosine Similarity](#Cosine-Similarity)\n",
    "    \n",
    "    \n",
    "* [Classification](#Classification)\n",
    "    - [Naive Bayes Classifier](#Naive-Bayes-Classifier)\n",
    "\n",
    "\n",
    "* [Clustering](#Clustering)\n",
    "    - [Hierarchical Clustering](#Hierarchical-Clustering)\n",
    "    \n",
    "Coming up soon -\n",
    " * K-Means Clustering\n",
    " * Latent Semantic Indexing\n",
    " * Singular Value Decomposition\n",
    " * PageRank for Text\n",
    " * Text Summarization\n",
    "\n",
    "--------------------------------------------------------------------------\n",
    "\n",
    "## Web Scraping - Extracting Text From Target Website\n",
    "\n",
    "This is pretty important because in practical use, often you'll find yourself extracting information from websites or other sources as you can't always count on having a neat, clean dataset handed to you.\n",
    "\n",
    "First, we start by importing the modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define our target website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_website = 'http://www.php.net'\n",
    "response = urllib.request.urlopen(target_website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the response from the target website, we need to extract the text out of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHP: Hypertext PreprocessorDownloadsDocumentationGet InvolvedHelpGetting StartedIntroductionA simple tutorialLanguage ReferenceBasic syntaxTypesVariablesConstantsExpressionsOperatorsControl StructuresFunctionsClasses and ObjectsNamespacesErrorsExceptionsGeneratorsReferences ExplainedPredefined VariablesPredefined ExceptionsPredefined Interfaces and ClassesContext options and parametersSupported Protocols and WrappersSecurityIntroductionGeneral considerationsInstalled as CGI binaryInstalled as an\n"
     ]
    }
   ],
   "source": [
    "html = response.read() #read the response sent from the target website\n",
    "soup = BeautifulSoup(html, \"html5lib\") #instantiate beautifulsoup object\n",
    "text = soup.get_text(strip = True) #extract the text\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Now, to process any sort of text, its important to \"tokenize\" the data. This basically means that we need to split the data into the words it consists of.\n",
    "\n",
    "For example, the text \"Aditya loves writing code\", after tokenizing, becomes [\"Aditya\", \"loves\", \"writing\", \"code\"]\n",
    "\n",
    "It's the mere splitting up of text into words. But nevertheless, there exists an NLTK library for it. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aditya', 'loves', 'writing', 'code']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = 'Aditya loves writing code'\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's as simple as tokenizing gets.\n",
    "\n",
    "----------------------\n",
    "\n",
    "## Stopword Removal\n",
    "\n",
    "Often its convenient to strip extremely common words from one's text corpus as very rarely do these words add any value to the information mining or retrieval process.\n",
    "\n",
    "These words are called stopwords. Examples include \"a\", \"an\", \"the\", \"in\" etc. Most queries retain their meaning even after stopword removal and are more \"content\" rich after this.\n",
    "\n",
    "Lucky for us, there's an NLTK module that consists of generally accepted stopwords so we will simply leverage this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it! Now to determine what english words are stopwords, we execute the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = list(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now that we have our stopwords, we can remove them from any text corpus we have, and perhaps this may improve our model's accuracy. But more on that later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now that we've removed our stopwords from our text corpus, there's still another factor that remains.\n",
    "\n",
    "The word's root.\n",
    "\n",
    "I'll explain with an example -\n",
    "\n",
    "Let's say you want to build the next Google - or any information retrieval system. \n",
    "\n",
    "Now, if one were to search for \"Boy waiting with dog\", an article titled \"Boys wait with dogs\" may be relevant. So how do we get an algorithm that recognizes this? Stemming.\n",
    "\n",
    "-----------------------------------------------\n",
    "\n",
    "\n",
    "## Word Stemming\n",
    "\n",
    "After stemming, our text corpus transforms from a set of words, to a set of terms. The difference between a word and a term is that a term is a meaningful entity in our text corpus. Words refer to the unfiltered dataset, while terms are the essence of it on which we make intelligent decisions.\n",
    "\n",
    "But we're getting ahead of ourselves - firstly, what is stemming?\n",
    "\n",
    "\"Stemming\" as the name suggests, is the idea of removing all excess parts of the word until you're left with the root of the word.\n",
    "\n",
    "Most english words have many variations - for example, the root word \"wait\" has \"waiting\", \"waited\", \"waits\" all as variants.\n",
    "\n",
    "In order to compress these multiple possibilities, we stem the words down to their root forms so that we can compare the words directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEMMED SENTENCE 1: boy wait dog  \n",
      "STEMMED SENTENCE 2: boy wait dog \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence_1 = 'Boy waiting with dog'\n",
    "sentence_1 = word_tokenize(sentence_1)\n",
    "sentence_1 = [i for i in sentence_1 if not i in list(stopwords.words('english'))]\n",
    "\n",
    "sentence_2 = 'Boys wait with dogs'\n",
    "sentence_2 = word_tokenize(sentence_2)\n",
    "sentence_2 = [i for i in sentence_2 if not i in list(stopwords.words('english'))]\n",
    "\n",
    "\n",
    "PS = PorterStemmer()\n",
    "\n",
    "stemmed_sentence_1 = ''\n",
    "stemmed_sentence_2 = ''\n",
    "\n",
    "for i in sentence_1:\n",
    "    stemmed_sentence_1 += PS.stem(i) + ' '\n",
    "\n",
    "for i in sentence_2:\n",
    "    stemmed_sentence_2 += PS.stem(i) + ' '\n",
    "\n",
    "print('STEMMED SENTENCE 1:',stemmed_sentence_1, '\\nSTEMMED SENTENCE 2:', stemmed_sentence_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that! Both those sentence resolve down to one root sentence! Of course, this required prior tokenizing AND stopword removal. \n",
    "\n",
    "Now we move on to the next topic -- Document Representation!\n",
    "\n",
    "---------------------\n",
    "\n",
    "\n",
    "## Document Representation\n",
    "\n",
    "### Boolean Matrix\n",
    "\n",
    "The boolean matrix is a simple way of representing a set of documents where each document has some text of its own.\n",
    "\n",
    "It represents this information in a table where each row defines a separate document, and each column defines the presence or absence of a specific **TERM**.\n",
    "\n",
    "So basically, collect all the terms, build a table with *D* number of rows (*D* = number of documents) and T number of columns (*T* = Number of terms)\n",
    "\n",
    "Then, for every *i*th row and *j*th column, if term *Tj* is present in document *Di*, mark 1, else mark 0.\n",
    "\n",
    "While this representation is quite handy in performing boolean queries, such as \"Find all documents that have term A AND term B but NOT term C OR term D\", it's limitations are many.\n",
    "\n",
    "There problems with this representation are -\n",
    "* Takes up a lot of space = O(T\\*D)\n",
    "* No information on frequency of terms per document.\n",
    "* No information on order of terms in a document.\n",
    "\n",
    "**As an example to illustrate its weakness, consider the results if one of the documents in your dataset was the English Oxford Dictionary. RIP Information Retrieval Accuracy, with this method.**\n",
    "\n",
    "So the next method!\n",
    "\n",
    "### Term Frequency Matrix or Bag Of Words Model\n",
    "\n",
    "Similar to Boolean Matrix except here we don't use a '1' or '0' notation -- instead, for every *i*th row and *j*th column, if term *Tj* is present in document *Di*, mark the frequency of *Tj* in *Di*, else mark 0.\n",
    "\n",
    "This gives us information on how many TIMES a certain term has appeared in a document, and thus it helps us rank documents on a relevance scale.\n",
    "\n",
    "This is critical in information querying and retrieval as otherwise we may not be able to decipher which document is most similar to a query.\n",
    "\n",
    "The problem of lack of order of terms still is present - but we'll find ways to get around that.\n",
    "\n",
    "### Complete Representation\n",
    "\n",
    "This is the ultimate method of representing documents. It's practically overpowered - this method stores both the frequency of terms for every document, and ALSO the indexes at which the term *Tj* occurs at document *Di*.\n",
    "\n",
    "The space complexity for this is O(sum of total terms in all documents) as every index corresponds to a term, and therefore the space taken by this model is much higher than anything else we've seen so far.\n",
    "\n",
    "But the cool thing about this is that with just this table it's possible to fully reproduce the initial documents so there is absolutely NO information loss.\n",
    "\n",
    "-------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "## IDF and TF-IDF\n",
    "\n",
    "IDF, stands for Inverse-Document-Frequency. The idea behind IDF is that sometimes, very frequent words may be irrelevant as we may be looking for a document highly specialized. It's similar to stopwords, except that the IDF helps us find our supposed stopwords in context to our query topic.\n",
    "\n",
    "For example, if one were to query \"Algorithms\", its likely that most resultant documents will contain the term \"computer\" so that term doesn't really contribute much with respect to other technical phrases like \"Data Structures\" or \"Time complexity\" etc.\n",
    "\n",
    "So we weight the terms by their importance, during TF-IDF (Term Frequency and Inverse Document Frequency) as an attempt to identify specialized documents in the niche.\n",
    "\n",
    "I'll skip the mathematics, but in summary, \n",
    "\n",
    "```code\n",
    "IDF = log2( total number of documents / (1 + number of documents that contain a given term) )\n",
    "```\n",
    "\n",
    "Thus, your end result will be a vector of length |T| (number of terms), with a floating point value at each index denoting the IDF value of that corresponding term at the same index.\n",
    "\n",
    "Here's how that looks in implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def term_frequency(term, document):\n",
    "    \"\"\"Here we return the number of times the parameter term appears in the parameter document\"\"\"\n",
    "    return document.count(term)\n",
    "\n",
    "def documents_containing_term(term, all_documents):\n",
    "    \"\"\"Here we return the number of documents that contain the given term\n",
    "    all_documents is a list of documents comprising of the text of each document in each index\"\"\"\n",
    "    return sum([1 for document in all_documents if term in document])\n",
    "\n",
    "def idf(term, all_documents):\n",
    "    \"\"\"Calculates the IDF value of the given term\"\"\"\n",
    "    return math.log( len(all_documents) / (1 + documents_containing_term(term, all_documents)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can calculate IDF values for each term, we weight the terms with their term frequency.\n",
    "\n",
    "We do this with TF-IDF, and the TF-IDF for a given term *Tj* in a document *Di* is calculated as -\n",
    "\n",
    "```code\n",
    "TF-IDF(*Tj*, *Di*) = TF(*Tj*, *Di*) * IDF(*Tj*)\n",
    "```\n",
    "\n",
    "And with that we can successfully weight our terms!\n",
    "\n",
    "Here's how that looks in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(term, document, all_documents):\n",
    "    \"\"\"Method to return TF-IDF of a given term in a given document\"\"\"\n",
    "    return tf(term, document) * idf(term, all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can successfully weight our terms, we're done with manipulating our text corpus (for now).\n",
    "\n",
    "Next follows, the concept of similarity.\n",
    "\n",
    "## Similarity\n",
    "\n",
    "Similarity is the idea of comparing two queries or sentences and determining how similar the two are. Ideally, you'd want to retrieve MOST similar results when given a query, right?\n",
    "\n",
    "Well, similarity helps you quantify this in numbers and we do this using the **Vector Space Model**.\n",
    "\n",
    "The idea is very simple if you read this next part slowly.\n",
    "\n",
    "*Start of important part.*\n",
    "\n",
    "Each sentence, is treated as a vector. Yes, I'm talking about the math, geometry vector.\n",
    "\n",
    "The co-ordinates of the vector are obtained by taking the term frequencies, as each term represents a dimension.\n",
    "\n",
    "Let me repeat that - **Each term represents a dimension.**\n",
    "\n",
    "Two terms would mean two dimensions (X and Y).\n",
    "\n",
    "Three terms would mean three dimensions (X, Y and Z).\n",
    "\n",
    "So let's say you have 2 sentences (after tokenizing, stemming and stopword removal) -\n",
    "* aditya play dog\n",
    "* aditya study\n",
    "\n",
    "Now, the terms here are (Aditya, play, dog, study). Let's sort them just to normalize them.\n",
    "\n",
    "The terms after sorting, are (aditya, dog, play, study).\n",
    "\n",
    "Now we can plot the two sentences in a 4-dimensional graph, by taking the coordinates from the term frequencies.\n",
    "\n",
    "Meaning, sentence 1 \"aditya play dog\" will have the coordinates (1, 1, 1, 0)\n",
    "\n",
    "And sentence 2 \"aditya study\" will have the coordinates (1, 0, 0, 1)\n",
    "\n",
    "**NOTE: This is term frequency. Not binary representation. If there were two terms in a sentence, the coordinate for that term would be 2. Not 1.**\n",
    "\n",
    "*End of important part. Congrats.*\n",
    "\n",
    "So now that we've represented our sentences (or text corpuses, i.e. documents) as vectors, we can find how similar they are in a few ways. The most common are -\n",
    "\n",
    "* [Euclidean Similarity](#Euclidean-Similarity)\n",
    "* [Cosine Similarity](#Cosine-Similarity)\n",
    "\n",
    "### Euclidean Similarity\n",
    "\n",
    "Literally. The. Euclidean. Distance. Formula.\n",
    "\n",
    "Given two points in an N-dimensional space, the distance between them is the square root of the sum of the difference of their squares.\n",
    "\n",
    "![title](https://wikimedia.org/api/rest_v1/media/math/render/svg/795b967db2917cdde7c2da2d1ee327eb673276c0)\n",
    "\n",
    "**NOTE: In the case of Euclidean Similarity, the smaller the value, the better as the two points must be closer.**\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "Literally. The. Dot. Product. Formula.\n",
    "\n",
    "Here we just take the dot product to see how similar the two vectors are, as the dot product produces a representation of one vector on the other.\n",
    "\n",
    "Here we just multiply the term frequency\n",
    "\n",
    "![title](https://wikimedia.org/api/rest_v1/media/math/render/svg/5bd0b488ad92250b4e7c2f8ac92f700f8aefddd5)\n",
    "\n",
    "**NOTE: In the case of Cosine Similarity, the larger the value, the better as the cosine function is decreasing, and secondly, the larger the value is, the closer the two points must be as the representation of one on the other is still large.** \n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "Well, now that we know how to manipulate documents, generate vectors, and even compare similarity between queries and documents - **We know enough to build a basic information retrieval system! Rejoice!**\n",
    "\n",
    "Now, moving further.\n",
    "\n",
    "A common task in NLP is classification.\n",
    "\n",
    "Now, classification is an umbrella term that covers many topics such as spam detection, credit card fraud analysis, sentiment analysis, and many more.\n",
    "\n",
    "So it's important to be able to classify documents to streamline searching, information retrieval and even to make educated decisions.\n",
    "\n",
    "# Classification\n",
    "\n",
    "There are many classifiers out there, but we'll start with the simple ones first. \n",
    "\n",
    "### Naive Bayes Classifier\n",
    "\n",
    "The Naive Bayes Classifier is a multiclass classifier that relies on Bayesian Probability in order to compute the class a document may belong to.\n",
    "\n",
    "The idea behind it is EXTREMELY simple, and I just want you to understand it because Python's Scikit-Learn module provides its own ready-to-use functions to build classifiers, so implementing them will be easy - understanding what's going on under the hood is the real goal.\n",
    "\n",
    "The Naive Bayes Classifier works by comparing the probability that a document *Di* may belong to a class *Cx* based on the likelihood of class *Cx* being the correct class, and the likelihood that the terms in *Di* belong to *Cx*\n",
    "\n",
    "Consequently, it does not factor in inter-term relationships, as it assumes each term uniquely and individually contributes to the probability of the document's final class.\n",
    "\n",
    "Personally, I love this diagram because it's pretty accurate in conveying how Bayes' Formula is used in Classification. I'd advise you to inspect the diagram carefully and re-read this section on the Naive Bayes all over again, as then it'll make the concept crystal clear.\n",
    "\n",
    "![Bayes Classifier](https://cdn-images-1.medium.com/max/1200/1*ZW1icngckaSkivS0hXduIQ.jpeg)\n",
    "\n",
    "As for the implementation, here goes -\n",
    "\n",
    "I'll be using Scikit-Learn's 20 news groups dataset, however you can use any labelled dataset you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.972742759796\n"
     ]
    }
   ],
   "source": [
    "\"\"\"These are categories from the 20 news groups dataset, ignore this next line if you're using your own dataset.\"\"\"\n",
    "topics = ['rec.autos', 'rec.sport.hockey', 'sci.space', 'soc.religion.christian', 'talk.politics.guns']\n",
    "\n",
    "training = fetch_20newsgroups(subset = 'train', categories = topics, shuffle = True, random_state = 42)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vec', TfidfVectorizer(stop_words = stopwords.words('english'))),\n",
    "    ('cla', MultinomialNB())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training.data, training.target, test_size = 0.2, random_state = 33)\n",
    "\n",
    "classifier.fit(X_train, y_train) # Here we give the classifier the prior probabilities.\n",
    "\n",
    "print('Accuracy:', classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! That's an insanely cool accuracy for our first classifier! Then again, our dataset was also nice - in practice, results like these may not come often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the query: weapons are not allowed in here\n",
      "Class Type: 4\n",
      "Class name: talk.politics.guns\n",
      "--------------------------------------------------\n",
      "For the query: thats a ferrari right?\n",
      "Class Type: 0\n",
      "Class name: rec.autos\n"
     ]
    }
   ],
   "source": [
    "query = ['weapons are not allowed in here']\n",
    "class_type = int(classifier.predict(query))\n",
    "print('For the query:',*query)\n",
    "print('Class Type:',class_type)\n",
    "print('Class name:', training.target_names[class_type])\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "query = ['thats a ferrari right?']\n",
    "class_type = int(classifier.predict(query))\n",
    "print('For the query:',*query)\n",
    "print('Class Type:',class_type)\n",
    "print('Class name:', training.target_names[class_type])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty sweet, right? It's that easy to build a classifier.\n",
    "\n",
    "Most machine learning models in Python are very simple to implement. Just instantiate the class, and call the ```fit()``` method, then the ```predict()``` method.\n",
    "\n",
    "What's hard is understanding how things work under the hood and leveraging that to manipulate your data to get better results.\n",
    "\n",
    "This is where you don't want stopwords messing with your accuracy so you remove them. Or you don't want the suffixes of variants of the same word confusing your model, so you stem them.\n",
    "\n",
    "It's part of the big picture.\n",
    "\n",
    "Now that you've got an idea on implementing classifiers, we check out the other side of NLP with ML.\n",
    "\n",
    "-----------------------\n",
    "\n",
    "# Clustering\n",
    "\n",
    "Clustering is critical if you want to identify documents similar to each other. Its usually an unsupervised learning problem, while classification is a supervised learning problem. \n",
    "\n",
    "Here's what that means in very simple terms -\n",
    "\n",
    "* A supervised learning problem is one where you tell the machine - \"Hi, this is the answer when your data looks like this. Now that you've got examples, learn from it.\"\n",
    "\n",
    "* An unsupervised learning problem is when you tell the machine - \"Hey, I don't know what this data means, take a look and tell me if you can find any pattern in there that I can't see yet.\"\n",
    "\n",
    "Basically, in an unsupervised problem, you don't tell the machine what its looking for - it finds that out itself. In a supervised problem, you tell the machine what to find, given what information.\n",
    "\n",
    "Now that we've cleared that up -- Clustering is often associated with unsupervised problems.\n",
    "\n",
    "\"Clustering\" is the idea of grouping up some data points and claiming that these data points are similar to each other under certain parameters. I'll explain as we progress.\n",
    "\n",
    "So let's look at some common clustering algorithms - \n",
    "* Hierarchical Clustering\n",
    "* K-Means Clustering\n",
    "\n",
    "Both of these are multipass - meaning they go over the data points repeatedly in order to cluster them.\n",
    "\n",
    "Now before we go further, it's important to be able to visualize this, so for now I'll only deal in two dimensions, X and Y.\n",
    "\n",
    "Once you understand how these algorithms work in two dimensions, it's easy to scale them up to N dimensions, using the vector space model. \n",
    "\n",
    "So, picture points like this -\n",
    "\n",
    "![title](https://cdn-images-1.medium.com/max/1180/1*Xo-oGP0Fb0217QuiWZrheQ.png)\n",
    "\n",
    "Now clearly upon inspection alone we know how many clusters there are, and which points belong to which clusters -- but how do we generalize this into an algorithm for a computer to learn too?\n",
    "\n",
    "Well, let's see two different methods.\n",
    "\n",
    "### Hierarchical Clustering\n",
    "\n",
    "The idea behind Hierarchical clustering is simple, we build a \"dendrogram\" which is basically just a tree that describes the order of clustering based on a chosen heuristic.\n",
    "\n",
    "These heuristics are -\n",
    "* Single Linkage\n",
    "* Complete Linkage\n",
    "\n",
    "The idea for all of them is the same, just the mathematical operation changes based on the heuristic.\n",
    "\n",
    "In the case of single linkage, here's the algorithm -\n",
    "* Repeat the following 3 steps until all points have found a cluster\n",
    "* Find the distance between every pair of points [takes O(n^2)]\n",
    "* Then, join the pair whose distance is smallest - this form the first cluster\n",
    "* Then recalculate the distance between all pairs -- except whenever you're considering a cluster's distance with a point, you take the smallest value among the distance between the external point and all internal points of the cluster.\n",
    "\n",
    "And that's it!\n",
    "\n",
    "In the case of Complete Linkage, re-read the algorithm above except replace the word \"smallest\" with the word \"largest\".\n",
    "\n",
    "And here's a nice and simple implementation of single linkage -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "\"\"\"Now we generate some text corpus, I'm hardcoding it here but in practice you'd want to take it out of denser documents.\"\"\"\n",
    "\n",
    "s1 = 'aditya boy'.split()\n",
    "s2 = 'ramesh father aditya'.split() \n",
    "s3 = 'bob and ramesh live india'.split()\n",
    "s4 = 'ramesh work office'.split()\n",
    "s5 = 'akash study college'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our text corpus, we identify the terms, and generate the term frequency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'color_list': ['b', 'b', 'b', 'b'],\n",
       " 'dcoord': [[0.0, 1.7320508075688772, 1.7320508075688772, 0.0],\n",
       "  [1.7320508075688772, 2.0, 2.0, 0.0],\n",
       "  [2.0, 2.2360679774997898, 2.2360679774997898, 0.0],\n",
       "  [2.2360679774997898, 2.4494897427831779, 2.4494897427831779, 0.0]],\n",
       " 'icoord': [[5.0, 5.0, 15.0, 15.0],\n",
       "  [10.0, 10.0, 25.0, 25.0],\n",
       "  [17.5, 17.5, 35.0, 35.0],\n",
       "  [26.25, 26.25, 45.0, 45.0]],\n",
       " 'ivl': [2, 1, 4, 5, 3],\n",
       " 'leaves': [1, 0, 3, 4, 2]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = sorted(list(set(s1 + s2 + s3 + s4 + s5)))\n",
    "\n",
    "# Generating the term frequency matrix.\n",
    "term_doc = [[0 for i in range(len(terms))] for j in range(5)]\n",
    "\n",
    "doc_no = 0\n",
    "\n",
    "# Iterating over the documents\n",
    "for i in [s1, s2, s3, s4, s5]:\n",
    "    term_doc[doc_no] = [i.count(j) for j in terms]\n",
    "    doc_no += 1\n",
    "\n",
    "# Type-casting it to a numpy array to use Scipy's hierarchical clustering module\n",
    "X = np.array(term_doc)\n",
    "\n",
    "linked = linkage(X, 'single')\n",
    "\n",
    "lo = 1\n",
    "hi = np.shape(X)[0] + 1\n",
    "\n",
    "labelList = range(lo, hi)\n",
    "\n",
    "plt.figure(figsize = (hi-lo, 5))\n",
    "dendrogram(linked,\n",
    "          orientation = 'top',\n",
    "          labels = labelList,\n",
    "          distance_sort = 'descending',\n",
    "          show_leaf_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that was a lotta code. Let's go through what we did.\n",
    "\n",
    "We generated the term frequency matrix and then type casted it to a numpy array.\n",
    "\n",
    "Then we instantiated our linkage object and defined it to use the heuristic - single linkage.\n",
    "\n",
    "We also gave this object our term frequency matrix so it can generate the vectors from the documents and compute the distances between every pair of vertices (documents).\n",
    "\n",
    "Then we defined the specifications for plotting our dendrogram using matplotlib's functions.\n",
    "\n",
    "And then we defined our dendrogram.\n",
    "\n",
    "Now for the magic -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAE1CAYAAACRPefNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x283949f5c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we go! Now, go back and check out the corresponding sentences, and you'll notice sentence 2 and 1 were pretty similar, while 3 was pretty out of place. (I mean, who's Bob? He's not even in the same country. ðŸ˜‚)\n",
    "\n",
    "But anyways, that's how you perform hierarchical clustering.\n",
    "\n",
    "Obviously, it's just as easy to perform Complete Linkage by changing \"```single```\" to \"```complete```\" in the instantiation of the linkage object. \n",
    "\n",
    "---------------------------------\n",
    "\n",
    "# Stay Tuned For More Updates\n",
    "\n",
    "Follow me on LinkedIn or GitHub to get a notification when I add more content to this mega blog post! ðŸ’™\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "Authored by Aditya Ramesh\n",
    "\n",
    "Follow me here -\n",
    "* My Linkedin - https://www.linkedin.com/in/adityaramesh1998\n",
    "* My Portfolio Website and Contact - https://www.rameshaditya.com/\n",
    "* My GitHub Profile - https://www.github.com/RameshAditya/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
